{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the top trending repositories of github\n",
    "\n",
    "Web scraping is the process of extracting and parsing data from websites in an automated fashion using a computer program. It's a useful technique for creating datasets for research and learning. Follow these steps to build a web scraping project from scratch using Python and its ecosystem of libraries:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick a website and describe your objective\n",
    "\n",
    "  -  Browse through different sites and pick on to scrape. Check the \"Project Ideas\" section for inspiration.\n",
    "  -  Identify the information you'd like to scrape from the site. Decide the format of the output CSV file.\n",
    "  -  Summarize your project idea and outline your strategy in a Juptyer notebook. Use the \"New\" button above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we are going to use https://github.com/topics\n",
    "- We will scrape the top 20 repositories of some topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the requests library to download web pages\n",
    "\n",
    "  -  Inspect the website's HTML source and identify the right URLs to download.\n",
    "  -  Download and save web pages locally using the requests library.\n",
    "  -  Create a function to automate downloading for different topics/search queries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install requests --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://github.com/topics'\n",
    "r = requests.get(url)\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('topics.html', 'w') as page:\n",
    "#     page.write(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Beautiful Soup to parse and extract information\n",
    "\n",
    "  -  Parse and explore the structure of downloaded web pages using Beautiful soup.\n",
    "  -  Use the right properties and methods to extract the required information.\n",
    "  -  Create functions to extract from the page into lists and dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install beautifulsoup4 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_divs = soup.find_all('div', {'class' : 'py-4'})\n",
    "topic_titles = []\n",
    "topic_urls = []\n",
    "topic_descs = []\n",
    "for div in topic_divs:\n",
    "    link = 'https://github.com' + div.find('a')['href']\n",
    "    topic_urls.append(link)\n",
    "\n",
    "    topic_info = div.find_all('p')\n",
    "\n",
    "    title = topic_info[0].text\n",
    "    topic_titles.append(title)\n",
    "\n",
    "    desc = topic_info[1].text.strip()\n",
    "    topic_descs.append(desc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = {\n",
    "    \"title\" : topic_titles,\n",
    "    \"description\" : topic_descs,\n",
    "    \"url\" : topic_urls\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pandas --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping topics from each trending repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_df(topic):\n",
    "    r = requests.get(topic)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    repos = soup.find_all('div', {'class' : 'd-flex flex-justify-between flex-items-start flex-wrap gap-2 my-3'})\n",
    "\n",
    "    repo_urls = []\n",
    "    repo_names = []\n",
    "    repo_users = []\n",
    "    repo_stars = []\n",
    "\n",
    "    for repo in repos:\n",
    "        repo_info = repo.findChildren('a', {'class' : \"Link\"})\n",
    "\n",
    "        user = repo_info[0].text.strip()\n",
    "        repo_users.append(user)\n",
    "\n",
    "        repo_name = repo_info[1].text.strip()\n",
    "        repo_names.append(repo_name)\n",
    "\n",
    "        repo_url = 'https://github.com/' + user+'/'+repo_name\n",
    "        repo_urls.append(repo_url)\n",
    "\n",
    "        stars = repo.findChild('span', {'id' : 'repo-stars-counter-star'}).text\n",
    "        repo_stars.append(stars)\n",
    "\n",
    "\n",
    "    repos_info = {\n",
    "    \"repository owner\" : repo_users,\n",
    "    \"repository name\" : repo_names,\n",
    "    \"repository url\" : repo_urls,\n",
    "    \"repository stars\" : repo_stars\n",
    "    }\n",
    "\n",
    "    repo_df = pd.DataFrame(repos_info)\n",
    "    return repo_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CSV file(s) with the extracted information\n",
    "\n",
    "  -  Create functions for the end-to-end process of downloading, parsing, and saving CSVs.\n",
    "  -  Execute the function with different inputs to create a dataset of CSV files.\n",
    "  -  Verify the information in the CSV files by reading them back using Pandas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting data from : 3d\n",
      "processing done for : 3d \n",
      "********************\n",
      "\n",
      "\n",
      "collecting data from : ajax\n",
      "processing done for : ajax \n",
      "********************\n",
      "\n",
      "\n",
      "collecting data from : algorithm\n",
      "processing done for : algorithm \n",
      "********************\n",
      "\n",
      "\n",
      "collecting data from : amphp\n",
      "processing done for : amphp \n",
      "********************\n",
      "\n",
      "\n",
      "collecting data from : android\n",
      "processing done for : android \n",
      "********************\n",
      "\n",
      "\n",
      "collecting data from : angular\n",
      "processing done for : angular \n",
      "********************\n",
      "\n",
      "\n",
      "collecting data from : ansible\n",
      "processing done for : ansible \n",
      "********************\n",
      "\n",
      "\n",
      "collecting data from : api\n",
      "processing done for : api \n",
      "********************\n",
      "\n",
      "\n",
      "collecting data from : arduino\n",
      "processing done for : arduino \n",
      "********************\n",
      "\n",
      "\n",
      "collecting data from : aspnet\n",
      "processing done for : aspnet \n",
      "********************\n",
      "\n",
      "\n",
      "collecting data from : atom\n",
      "processing done for : atom \n",
      "********************\n",
      "\n",
      "\n",
      "collecting data from : awesome\n",
      "processing done for : awesome \n",
      "********************\n",
      "\n",
      "\n",
      "collecting data from : aws\n",
      "processing done for : aws \n",
      "********************\n",
      "\n",
      "\n",
      "collecting data from : azure\n",
      "processing done for : azure \n",
      "********************\n",
      "\n",
      "\n",
      "collecting data from : babel\n",
      "processing done for : babel \n",
      "********************\n",
      "\n",
      "\n",
      "collecting data from : bash\n",
      "processing done for : bash \n",
      "********************\n",
      "\n",
      "\n",
      "collecting data from : bitcoin\n",
      "processing done for : bitcoin \n",
      "********************\n",
      "\n",
      "\n",
      "collecting data from : bootstrap\n",
      "processing done for : bootstrap \n",
      "********************\n",
      "\n",
      "\n",
      "collecting data from : bot\n",
      "processing done for : bot \n",
      "********************\n",
      "\n",
      "\n",
      "collecting data from : c\n",
      "processing done for : c \n",
      "********************\n",
      "\n",
      "\n",
      "collecting data from : chrome\n",
      "processing done for : chrome \n",
      "********************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for topic in topics[\"url\"][:21]:\n",
    "    df_name = topic.split('/')[-1]\n",
    "    print(f\"collecting data from : {df_name}\")\n",
    "    repo_df = get_repo_df(topic)\n",
    "    repo_df.to_csv(f\"csv-files/{df_name}.csv\", index=None)\n",
    "    print(f\"processing done for : {df_name} \")\n",
    "    print(\"*\" * 20 + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
